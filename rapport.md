Le prémier but était de traiter une collection de documents en démontrant la loi de Zipf.

D'abord, nous avons fait le rapatriement et la préparation des données. Dans le fichier split_cacm3.py, la fonction "ExtractionDesFichiers" découpe le fichier cacm.all en une liste de fichier dans le répertoire "files/collection/". Chaque fichier CACM-XX contient les données d'une publication.

Le fichier tokenize_cacm.py ouvre les fichiers CACM-XX.all un à un et ne garde que les mot qui commencent par une lettre et qui ne contiennent ensuite que des lettres ou des chiffres, puis les écrit dans un fichier avec le même nom mais avec extension .tok dans le répertoire "files/colletion_tokens/". On boucle sur chaque fichier de "files/collection/" et pour chaque fichier, on découpe la ligne en tokens et boucle sur ces tokens, et on les écrit dans une ligne dans le nouveau fichier.

Le fichier zipf.py prend tous les fichiers CACM-XX.tok et calcule la fréquence d'apparition avec un dictionnaire python, la clé est la chaîne de caractères et la valeur est son nombre d'occurrence. Après, on calcule la taille du vocabulaire et la valeur lambda théorique. Pour faire ça, on boucle sur les fichiers .tok et pour chaque ligne/mot dans un fichier, on crée une valeur dans le dictionnaire, si la clé existe déjà, on incrémente sa valeur. Puis, on trie le dictionnaire par ordre décroissant de valeur et on calcule la taille du dictionnaire, on boucle sur chaque clé du dictionnaire et calcule la valeur lambda.

Le fichier create_dict.py génére un dictionnaire python qui contient comme clé le nom de fichier, et comme valeur un autre dictionnaire qui contient les mots qui apparaissent dans ce fichier avec leur nombre d'occurrence, avec l'application de l'anti-dictionnaire et la troncature des termes en utilisant la troncature de Porter.  Pour chaque fichier .tok, on boucle sur les tokens, applique la troncature, applique l'anti-dictionnaire et ajoute le mot dans le dictionnaire correspondante à valeur du fichier. Si le mot est déjà dans le dictionnaire, on incrémente sa valeur.

Le fichier create_vocabulaire.py construit un vocabulaire à partir de la collection précédente en considérant tous les termes qui apparaissent au moins une fois dans le corpus de documents, le résultat stocké dans le fichier vocabulaire.json. On utilise un dictionnaire python qui, pour chaque terme, stocke la valeur 0. On reprend la structure générée aux fichier create_dict.py. Puis, on calcule le df_i pour chaque terme t_i du vocabulaire et stocke dans le même dictionnaire. Enfin, on met chaque valeur à jour pour calculer le idf_i=ln(N/df_i). Pour faire tout ça, on boucle sur chaque valeur du dictionnaire représentant les tokens des fichiers et on l'ajoute dans un dictionnaire avec le valeur 0. On boucle à nouveau sur ce dictionnaire précédent et calcule le idf_i à partir du vocabulaire générée et la fréquence de chaque mot, en générant un autre dictionnaire dans vocabulaire.json

Le fichier create_vecteur.py construit la représentation vectorielle de chaque document d'après le modèle vectoriel de Salton avec tf_idf et nd=1. Chaque document est représenté par un dictionnaire de couples (terme, tf_idf du terme dans document). Nous obtenons donc, un gros dictionnaire qui stocke tous les vecteurs au sens recherche d'information des documents, avec l'identifiant de document comme clé et la représentation de son vecteur comme valeur. Pour chaque document et sa respective valeur du dictionnaire de fréquences de mot, on crée un nouveau dictionnaire comme clé le nom du fichier et comme valeur un autre dictionnaire représentant un de ses tokens et le tf_idf calculé.

Le fichier create_inverse.py construit l'index inversé de ses termes, à partir du vocabulaire et des vecteur des documents, et le sauvegarde dans le fichier index_inverse.json. On boucle sur chaque mot du vocabulaire et crée un nouveau dictionnaire, où la valeur pour le pair token et document est égal à valeur correspondant au pair document et token du index original.

Le fichier create_norme.py prépare les vecteurs pour le traitement des requêtes en calculant la norme des vecteurs avec la pondération tf_idf. C'est calculé par la racine carrée des valeurs de chaque dimension du vecteurs. On boucle sur le vecteur original et pour chaque tuple, on ajoute la valeur de la pondération calculée dans un nouveau dictionnaire vecteur_normalise.json.

Le fichier create_modele_vectoriel.py traite les requêtes en calculant la correspondance RSV avec le produit scalaire des vecteur du document et de la requête divisé par le produit des leurs normes. La requête dans le fichier "requete.txt" passe pour plusieurs étapes, les mêmes qu'on a été applies pour les documents du corpus. On tokenize, on applique l'anti-dictionnaire et la troncature, on crée le dictionnaire avec la fréquence tf pour chaque mot et on le multiplie par idf. À la fin on a le modèle vectoriel pour la requête.
